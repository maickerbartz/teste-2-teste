{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraphNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached langgraph-0.2.74-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (0.3.36)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Using cached langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.38-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.12-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain)\n",
      "  Using cached numpy-2.2.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchainhub) (24.2)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langchain-core) (4.12.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached propcache-0.2.1-cp313-cp313-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
      "  Using cached msgpack-1.1.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\administrador\\desktop\\teste_git\\teste_ollama\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Using cached langgraph-0.2.74-py3-none-any.whl (151 kB)\n",
      "Using cached langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Using cached aiohttp-3.11.12-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Using cached langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
      "Using cached langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
      "Using cached numpy-2.2.3-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Using cached SQLAlchemy-2.0.38-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached msgpack-1.1.0-cp313-cp313-win_amd64.whl (75 kB)\n",
      "Using cached multidict-6.1.0-cp313-cp313-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.2.1-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl (315 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: types-requests, python-dotenv, propcache, numpy, mypy-extensions, multidict, msgpack, marshmallow, httpx-sse, greenlet, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, langchainhub, aiosignal, pydantic-settings, langgraph-sdk, dataclasses-json, aiohttp, langgraph-checkpoint, langchain-text-splitters, langgraph, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 attrs-25.1.0 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 httpx-sse-0.4.0 langchain-0.3.19 langchain-community-0.3.17 langchain-text-splitters-0.3.6 langchainhub-0.1.21 langgraph-0.2.74 langgraph-checkpoint-2.0.16 langgraph-sdk-0.1.51 marshmallow-3.26.1 msgpack-1.1.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-2.2.3 propcache-0.2.1 pydantic-settings-2.7.1 python-dotenv-1.0.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchain langchain-community langchainhub langchain-core \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%docker exec -it ollama ollama run qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro! Vou explicar o que é um LLM (Large Language Model) de forma simples e passo a passo.\\n\\n1. **Modelos de Linguagem**: Primeiro, é importante entender que os modelos de linguagem são ferramentas inteligentes capazes de gerar texto com base em entradas fornecidas por usuários ou sistemas.\\n\\n2. **Tamanho dos Modelos**: \"Large\" refere-se a um modelo com uma grande quantidade de parâmetros (geralmente bilhões ou trilhões). Isso aumenta significativamente suas capacidades e domínios de aplicação, mas também requer mais recursos computacionais para treinamento e execução.\\n\\n3. **LLM**: Portanto, um LLM é um modelo de linguagem grande, que significa que ele foi treinado com uma vasta quantidade de dados textuais, permitindo-lhe compreender e gerar texto natural em múltiplos idiomas e temas complexos.\\n\\n4. **Capacidades**: Estes modelos são capazes de:\\n   - Geração de texto (por exemplo, resumos, contas de história, emails).\\n   - Resolução de problemas.\\n   - Conversação e interação natural com os usuários.\\n   - Análise de sentimentos em textos.\\n\\n5. **Exemplos**: Alguns exemplos famosos de LLM incluem a GPT (Generative Pre-trained Transformer) da OpenAI, a BERT (Bidirectional Encoder Representations from Transformers) e modelos similares criados por outras empresas ou organizações.\\n\\n6. **Considerações**: Embora extremamente úteis e avançados, os LLM também enfrentam desafios como necessidade de computação poderosa, possíveis problemas de coerência em texto gerado (especialmente com modelos mais antigos) e questões éticas relacionadas à confiabilidade do texto produzido.\\n\\nPortanto, um LLM é essencialmente um modelo de linguagem grande que demonstra habilidades avançadas na geração e compreensão de texto natural.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Questão: {question}\n",
    "\n",
    "Resposta: Vamos pensar passo a passo.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"qwen2.5\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \" você pode explicar o que é o LLM?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro! Vamos resolver essa equação passo a passo:\\n\\nEquação: 2x + 3 = 10\\n\\nPasso 1: Subtrai 3 de ambos os lados para isolar o termo com x.\\n         2x + 3 - 3 = 10 - 3\\n         2x = 7\\n\\nPasso 2: Divide ambos os lados por 2 para resolver para x.\\n         2x ÷ 2 = 7 ÷ 2\\n         x = 7/2 ou x = 3.5\\n\\nEntão, a solução da equação é x = 3.5.\\n\\nVocê pode verificar essa resposta substituindo x = 3.5 na equação original:\\n2(3.5) + 3 = 10\\n7 + 3 = 10\\n10 = 10\\n\\nIsto confirma que nossa solução está correta.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"Pode me resolver a equação 2x+3=10?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\" Representa o estado do processo de avaliação da redação \"\"\"\n",
    "    essay: str\n",
    "    relevance_score: float\n",
    "    grammar_score: float\n",
    "    structure_score: float\n",
    "    depth_score: float\n",
    "    final_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(content: str) -> float:\n",
    "    \"\"\"Extrai a pontuação numérica da resposta do LLM.\"\"\"\n",
    "    match = re.search(r'Pontuação:\\s*(\\d+(\\.\\d+)?)', content)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    raise ValueError(f\"Não foi possível extrair a pontuação de: {content}\")\n",
    "\n",
    "def check_relevance(state: State) -> State:\n",
    "    \"\"\"Verifica a relevância da redação.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Analise a relevância da seguinte redação em relação ao tema dado, prezando pela excelência da Língua Portuguesa. \"\n",
    "        \"Forneça uma pontuação de relevância entre 0 e 1. \"\n",
    "        \"Sua resposta deve começar com 'Pontuação: ' seguida da pontuação numérica, \"\n",
    "        \"depois forneça sua explicação.\\n\\nRedação: {essay}\"\n",
    "    )\n",
    "    result = model.invoke(prompt.format(essay=state[\"essay\"]))\n",
    "    try:\n",
    "        state[\"relevance_score\"] = extract_score(result.content)\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro em check_relevance: {e}\")\n",
    "        state[\"relevance_score\"] = 0.0\n",
    "    return state\n",
    "\n",
    "def check_grammar(state: State) -> State:\n",
    "    \"\"\"Verifica a gramática da redação.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Analise a gramática da Língua Portuguesa na seguinte redação. \"\n",
    "        \"Forneça uma pontuação de gramática entre 0 e 1. \"\n",
    "        \"Sua resposta deve começar com 'Pontuação: ' seguida da pontuação numérica, \"\n",
    "        \"depois forneça sua explicação.\\n\\nRedação: {essay}\"\n",
    "    )\n",
    "    result = model.invoke(prompt.format(essay=state[\"essay\"]))\n",
    "    try:\n",
    "        state[\"grammar_score\"] = extract_score(result.content)\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro em check_grammar: {e}\")\n",
    "        state[\"grammar_score\"] = 0.0\n",
    "    return state\n",
    "\n",
    "def analyze_structure(state: State) -> State:\n",
    "    \"\"\"Analisa a estrutura da redação.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Analise a estrutura de acordo com a normal culta da Língua Portuguesa na seguinte redação. \"\n",
    "        \"Forneça uma pontuação de estrutura entre 0 e 1. \"\n",
    "        \"Sua resposta deve começar com 'Pontuação: ' seguida da pontuação numérica, \"\n",
    "        \"depois forneça sua explicação.\\n\\nRedação: {essay}\"\n",
    "    )\n",
    "    result = llm.invoke(prompt.format(essay=state[\"essay\"]))\n",
    "    try:\n",
    "        state[\"structure_score\"] = extract_score(result.content)\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro em analyze_structure: {e}\")\n",
    "        state[\"structure_score\"] = 0.0\n",
    "    return state\n",
    "\n",
    "def evaluate_depth(state: State) -> State:\n",
    "    \"\"\"Avalia a profundidade de análise na redação.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Avalie a profundidade de análise na seguinte redação. \"\n",
    "        \"Forneça uma pontuação de profundidade entre 0 e 1. \"\n",
    "        \"Sua resposta deve começar com 'Pontuação: ' seguida da pontuação numérica, \"\n",
    "        \"depois forneça sua explicação.\\n\\nRedação: {essay}\"\n",
    "    )\n",
    "    result = model.invoke(prompt.format(essay=state[\"essay\"]))\n",
    "    try:\n",
    "        state[\"depth_score\"] = extract_score(result.content)\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro em evaluate_depth: {e}\")\n",
    "        state[\"depth_score\"] = 0.0\n",
    "    return state\n",
    "\n",
    "def calculate_final_score(state: State) -> State:\n",
    "    \"\"\"Calcula a pontuação final com base nas pontuações dos componentes individuais.\"\"\"\n",
    "    state[\"final_score\"] = (\n",
    "        state[\"relevance_score\"] * 0.3 +\n",
    "        state[\"grammar_score\"] * 0.2 +\n",
    "        state[\"structure_score\"] * 0.2 +\n",
    "        state[\"depth_score\"] * 0.3\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o StateGraph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Adiciona nós ao grafo\n",
    "workflow.add_node(\"check_relevance\", check_relevance)\n",
    "workflow.add_node(\"check_grammar\", check_grammar)\n",
    "workflow.add_node(\"analyze_structure\", analyze_structure)\n",
    "workflow.add_node(\"evaluate_depth\", evaluate_depth)\n",
    "workflow.add_node(\"calculate_final_score\", calculate_final_score)\n",
    "\n",
    "# Define e adiciona arestas condicionais\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_relevance\",\n",
    "    lambda x: \"check_grammar\" if x[\"relevance_score\"] > 0.5 else \"calculate_final_score\"\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_grammar\",\n",
    "    lambda x: \"analyze_structure\" if x[\"grammar_score\"] > 0.6 else \"calculate_final_score\"\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_structure\",\n",
    "    lambda x: \"evaluate_depth\" if x[\"structure_score\"] > 0.7 else \"calculate_final_score\"\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_depth\",\n",
    "    lambda x: \"calculate_final_score\"\n",
    ")\n",
    "\n",
    "# Define o ponto de entrada\n",
    "workflow.set_entry_point(\"check_relevance\")\n",
    "\n",
    "# Define o ponto de saída\n",
    "workflow.add_edge(\"calculate_final_score\", END)\n",
    "\n",
    "\n",
    "# Compila o grafo\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_essay(essay: str) -> dict:\n",
    "    \"\"\"Avalia a redação fornecida usando o fluxo de trabalho definido.\"\"\"\n",
    "    initial_state = State(\n",
    "        essay=essay,\n",
    "        relevance_score=0.0,\n",
    "        grammar_score=0.0,\n",
    "        structure_score=0.0,\n",
    "        depth_score=0.0,\n",
    "        final_score=0.0\n",
    "    )\n",
    "    result = app.invoke(initial_state)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_essay = \"\"\"\n",
    "O Impacto dos Agentes de Inteligência Artificial e Fluxos de Trabalho Agentes com Multi-ferramentas na Sociedade Moderna\n",
    "\n",
    "\n",
    "A Inteligência Artificial (IA) tem experimentado avanços significativos nas últimas décadas, e uma de suas manifestações mais revolucionárias é o desenvolvimento de agentes de IA e fluxos de trabalho agentes com multi-ferramentas. Estes agentes autônomos, capazes de aprender, adaptar-se e tomar decisões independentes, estão transformando a maneira como interagimos com a tecnologia e influenciando diversos setores da sociedade. Este ensaio explora os efeitos profundos desses agentes na sociedade moderna, discutindo seus benefícios, aplicações práticas e os desafios que apresentam.\n",
    "\n",
    "No campo da saúde, os agentes de IA estão desempenhando um papel crucial na melhoria dos cuidados aos pacientes e na eficiência operacional das instituições médicas. Sistemas avançados como o DeepMind Health, do Google, utilizam agentes de IA para analisar imagens médicas, detectando sinais precoces de doenças como câncer e retinopatia diabética com precisão impressionante. Além disso, agentes com multi-ferramentas podem integrar dados de diferentes fontes, como registros eletrônicos de saúde, resultados de exames laboratoriais e informações genômicas, para oferecer diagnósticos mais precisos e planos de tratamento personalizados.\n",
    "\n",
    "Durante a pandemia de COVID-19, agentes de IA foram empregados para modelar a propagação do vírus, prever surtos e otimizar a distribuição de recursos médicos. Esses agentes ajudaram governos e instituições de saúde a tomar decisões informadas sobre medidas de contenção e alocação de vacinas, demonstrando o potencial dos fluxos de trabalho agentes em situações de crise.\n",
    "\n",
    "No setor financeiro, os agentes de IA estão redefinindo a análise de risco, detecção de fraude e gestão de investimentos. Robôs consultores (robo-advisors) utilizam algoritmos de IA para fornecer aconselhamento financeiro personalizado, considerando objetivos individuais, tolerância ao risco e condições de mercado. Empresas como a BlackRock e Vanguard incorporam agentes de IA em suas estratégias de investimento, permitindo decisões mais ágeis e baseadas em dados.\n",
    "\n",
    "Além disso, agentes de IA com fluxos de trabalho agentes podem interagir com múltiplas ferramentas financeiras, monitorando transações em tempo real, analisando padrões de comportamento e identificando atividades suspeitas. Isso não apenas protege os consumidores, mas também fortalece a integridade do sistema financeiro global.\n",
    "\n",
    "A indústria de transporte está passando por uma transformação significativa com a introdução de veículos autônomos e sistemas inteligentes de gerenciamento de tráfego. Agentes de IA são o cérebro por trás de carros autônomos, interpretando dados de sensores, câmeras e radares para tomar decisões de direção seguras. Empresas como Waymo, Uber e Tesla estão na vanguarda dessa tecnologia, prometendo reduzir acidentes causados por erro humano e melhorar a eficiência das viagens.\n",
    "\n",
    "Em cidades inteligentes, agentes de IA gerenciam semáforos adaptativos, sistemas de transporte público e serviços de compartilhamento de veículos. Por exemplo, em Singapura, agentes inteligentes coordenam o fluxo de tráfego em tempo real, reduzindo congestionamentos e emissões de gases poluentes.\n",
    "\n",
    "Na educação, agentes de IA estão promovendo uma revolução no ensino e aprendizado. Plataformas educacionais inteligentes utilizam agentes para adaptar currículos às necessidades individuais dos estudantes. Esses sistemas analisam o desempenho, identificam áreas de dificuldade e ajustam o conteúdo para otimizar o aprendizado.\n",
    "\n",
    "Por exemplo, o sistema Knewton utiliza agentes de IA para fornecer recomendações personalizadas de estudo, enquanto o Squirrel AI, na China, oferece tutoria adaptativa para milhões de estudantes. Esses agentes permitem que os educadores atendam às necessidades específicas de cada aluno, promovendo um aprendizado mais eficaz e inclusivo.\n",
    "\n",
    "Na indústria e manufatura, os agentes de IA estão otimizando processos de produção, manutenção preditiva e gerenciamento da cadeia de suprimentos. Robôs industriais equipados com agentes inteligentes podem adaptar-se a diferentes tarefas, aprender com erros e colaborar com trabalhadores humanos em ambientes de fábrica.\n",
    "\n",
    "A Siemens, por exemplo, implementa agentes de IA em suas fábricas para monitorar o desempenho de máquinas, prever falhas e programar manutenções antes que ocorram interrupções. Isso resulta em maior eficiência, redução de custos e melhoria na qualidade dos produtos.\n",
    "\n",
    "No setor de serviços, agentes de IA estão revolucionando o atendimento ao cliente. Chatbots e assistentes virtuais utilizam processamento de linguagem natural para interagir com clientes, resolver problemas comuns e encaminhar questões mais complexas a agentes humanos. Isso melhora a experiência do cliente e permite que as empresas atendam a um volume maior de consultas com eficiência.\n",
    "\n",
    "Empresas como a Amazon e a Apple implementam agentes de IA em seus serviços de suporte, oferecendo assistência 24 horas por dia e personalizando as interações com base no histórico do cliente.\n",
    "\n",
    "Apesar dos benefícios significativos, a adoção de agentes de IA e fluxos de trabalho agentes apresenta desafios complexos. A automação avançada ameaça substituir empregos em diversos setores, desde manufatura até serviços profissionais. Estudos do Fórum Econômico Mundial indicam que milhões de empregos podem ser afetados, exigindo políticas robustas de requalificação e educação para preparar a força de trabalho para novas oportunidades.\n",
    "\n",
    "Questões éticas também são centrais neste debate. A tomada de decisões autônomas por agentes de IA levanta preocupações sobre responsabilidade legal em casos de falhas ou decisões prejudiciais. Além disso, algoritmos podem perpetuar vieses existentes se treinados em dados não representativos, levando a discriminação em áreas como contratação, crédito e justiça criminal.\n",
    "\n",
    "A privacidade dos dados é outra área crítica. Agentes de IA dependem de grandes quantidades de dados pessoais para aprender e operar eficientemente. Garantir que esses dados sejam coletados e utilizados de maneira ética, respeitando regulamentos como a Lei Geral de Proteção de Dados (LGPD) no Brasil, é essencial para manter a confiança do público.\n",
    "\n",
    "A crescente dependência de agentes de IA também levanta preocupações sobre segurança cibernética. Sistemas autônomos podem ser vulneráveis a ataques, manipulações e falhas sistêmicas. Incidentes envolvendo veículos autônomos ou sistemas financeiros automatizados podem ter consequências significativas, destacando a necessidade de protocolos de segurança robustos.\n",
    "\n",
    "Além disso, há o risco de dependência excessiva da tecnologia, onde habilidades humanas podem ser negligenciadas ou perdidas. É importante equilibrar a adoção de agentes de IA com o desenvolvimento contínuo das capacidades humanas e o pensamento crítico.\n",
    "\n",
    "O futuro dos agentes de IA é promissor, com avanços contínuos em áreas como processamento de linguagem natural, aprendizado por reforço e redes neurais profundas. A integração de agentes de IA com tecnologias emergentes como computação quântica e Internet das Coisas (IoT) ampliará ainda mais suas capacidades.\n",
    "\n",
    "Iniciativas como o AutoGPT e o BabyAGI representam passos em direção a agentes de IA capazes de aprender e se adaptar de forma autônoma, executando tarefas complexas sem intervenção humana constante. Esses sistemas têm o potencial de revolucionar setores inteiros, mas também exigem uma consideração cuidadosa dos impactos sociais e éticos.\n",
    "\n",
    "Em conclusão, os agentes de IA e fluxos de trabalho agentes com multi-ferramentas estão remodelando a sociedade moderna, oferecendo soluções inovadoras para desafios complexos e melhorando a eficiência em diversos setores. Seus benefícios são inegáveis, desde cuidados de saúde aprimorados até transportes mais seguros e educação personalizada.\n",
    "\n",
    "No entanto, esses avanços vêm acompanhados de desafios significativos que exigem atenção cuidadosa. A sociedade deve abordar questões de emprego, ética, privacidade e segurança de dados para garantir que a integração de agentes de IA beneficie a todos de maneira equitativa.\n",
    "\n",
    "À medida que avançamos para um futuro cada vez mais interconectado e orientado pela IA, é responsabilidade coletiva de governos, empresas, educadores e cidadãos trabalhar juntos. Devemos promover uma abordagem equilibrada que valorize a inovação tecnológica ao mesmo tempo em que protege os valores humanos fundamentais, assegurando que os agentes de IA sirvam como ferramentas para o bem-estar e progresso da humanidade.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Avalia a redação de exemplo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgrade_essay\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_essay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Converte as pontuações de 0-1 para 0-10\u001b[39;00m\n\u001b[0;32m      5\u001b[0m final_score \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mgrade_essay\u001b[1;34m(essay)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Avalia a redação fornecida usando o fluxo de trabalho definido.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m State(\n\u001b[0;32m      4\u001b[0m     essay\u001b[38;5;241m=\u001b[39messay,\n\u001b[0;32m      5\u001b[0m     relevance_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     final_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2124\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2124\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1779\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1779\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Desktop\\Teste_Git\\teste_ollama\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mcheck_relevance\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke(prompt\u001b[38;5;241m.\u001b[39mformat(essay\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messay\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_score(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErro em check_relevance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'",
      "\u001b[0mDuring task with name 'check_relevance' and id '3d6eba3d-14eb-756f-f48b-f359e3e6e040'"
     ]
    }
   ],
   "source": [
    "# Avalia a redação de exemplo\n",
    "result = grade_essay(sample_essay)\n",
    "\n",
    "# Converte as pontuações de 0-1 para 0-10\n",
    "final_score = result['final_score'] * 10\n",
    "relevance_score = result['relevance_score'] * 10\n",
    "grammar_score = result['grammar_score'] * 10\n",
    "structure_score = result['structure_score'] * 10\n",
    "depth_score = result['depth_score'] * 10\n",
    "\n",
    "# Exibe os resultados\n",
    "print(f\"Pontuação Final da Redação: {final_score:.2f}/10\\n\")\n",
    "print(f\"Pontuação de Relevância: {relevance_score:.2f}/10\")\n",
    "print(f\"Pontuação de Gramática: {grammar_score:.2f}/10\")\n",
    "print(f\"Pontuação de Estrutura: {structure_score:.2f}/10\")\n",
    "print(f\"Pontuação de Profundidade: {depth_score:.2f}/10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teste_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
